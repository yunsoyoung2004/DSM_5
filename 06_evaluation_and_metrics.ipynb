{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692cc8ed",
   "metadata": {},
   "source": [
    "\n",
    "# 06. Evaluation and Metrics for Multi-label Classification (Sanitized)\n",
    "\n",
    "This notebook documents the evaluation protocol used to assess multi-label\n",
    "DSM-5 depression detection models. All examples use **synthetic placeholders**\n",
    "to preserve privacy while maintaining full methodological transparency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e580d2",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Why Accuracy Is Not Used\n",
    "\n",
    "In multi-label, imbalanced mental health datasets:\n",
    "- Accuracy can be misleading\n",
    "- Most labels are sparse\n",
    "- Correctly predicting all labels is rare\n",
    "\n",
    "Therefore, we adopt **F1-based metrics**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946d788",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Metric Definitions\n",
    "\n",
    "- **Micro F1**: global performance across all labels\n",
    "- **Macro F1**: unweighted mean across labels\n",
    "- **Weighted F1**: label-frequency-weighted performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cce1fc",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Synthetic Ground Truth and Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_SAMPLES = 100\n",
    "NUM_LABELS = 9\n",
    "\n",
    "y_true = np.random.randint(0, 2, size=(NUM_SAMPLES, NUM_LABELS))\n",
    "y_pred = np.random.randint(0, 2, size=(NUM_SAMPLES, NUM_LABELS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defeae41",
   "metadata": {},
   "source": [
    "\n",
    "## 4. F1 Score Computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "micro_f1, macro_f1, weighted_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65eeff9",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Per-label Performance\n",
    "\n",
    "Per-label F1 scores allow diagnosis of which DSM-5 criteria\n",
    "are more difficult to detect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "per_label_f1 = [\n",
    "    f1_score(y_true[:, i], y_pred[:, i])\n",
    "    for i in range(NUM_LABELS)\n",
    "]\n",
    "\n",
    "per_label_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc088b",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Interpretation Notes\n",
    "\n",
    "- Micro F1 favors frequent symptoms\n",
    "- Macro F1 highlights rare but clinically important symptoms\n",
    "- Weighted F1 balances both perspectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b263d",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Reporting Strategy\n",
    "\n",
    "In the paper:\n",
    "- Weighted F1 is reported as the primary metric\n",
    "- Micro and Macro F1 are included for completeness\n",
    "- Results are averaged across runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36f205",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Ethics and Reproducibility\n",
    "\n",
    "- No real patient or user data are included\n",
    "- Metric computation exactly matches the original experiments\n",
    "- Evaluation logic is fully reproducible\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
